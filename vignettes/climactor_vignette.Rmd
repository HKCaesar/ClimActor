---
title: "The climactor guide"
author: "Data-Driven Lab"
output: rmarkdown::html_vignette
params:
  cdpname: name
vignette: >
  %\VignetteIndexEntry{The ClimActor guide}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(climactor)
```

As more non-state actors commit to climate action, new initiatives and databases recording such commitments have also become more commonplace. Many actors commit to multiple initiatives and appear in more than one database, yet show up across databases with slightly different names. This discrepancy makes data cleaning and wrangling more difficult than it should be and can result in over-counting of actor's climate commitments if not dealt with appropriately. 

The \code{climactor} package provides a workflow to efficiently standardize actor names across different databases and clean climate commitment data when analyzing non-state actors' climate commitments. 

## Data

The data used in this vignette comes from the Carbon Disclosure Project's 2018-2019 City-wide emissions data. The data can be downloaded [here](https://data.cdp.net/Emissions/2018-2019-City-wide-emissions/k7qn-m6i9). Metadata of the dataset is also documented in the link above. 

```{r, include = FALSE}
path <- system.file("extdata", "2018_-_2019_City-wide_emissions.csv", package = "climactor", mustWork = T)
cdp <- read.csv(path, stringsAsFactors = F,
                as.is = T, encoding = "UTF-8")
```

With data read in (as `cdp`)

``` {r}
dim(cdp)

# Quick peek at the data
knitr::kable(head(cdp[, 1:5]))
```

Apart from the data you wish to clean, two other important datasets would be the key dictionary (`key_dict`) and country dictionary (`country_dict`). These datasets can be accessed once the `climactor` package has been called with either `library(climactor)` or `require(climactor)`. For more details on the two dictionaries, you can use `?key_dict` and `?country_dict` for the full data documentation.  

```{r}
knitr::kable(head(key_dict[, 1:5]))
knitr::kable(head(country_dict[, 1:5]))
```

## Data Preparation functions

Naturally, the first step in any data cleaning pipeline would be to read in and prepare the data.

Within the `climactor` workflow the first important step comes when the data is read in. For the later string matching aspects of the workflow to work more accurately, the data should ideally be read in or converted to **UTF-8** encoding. **UTF-8** is the preferred encoding when working with strings and both the `key_dict` and `country_dict` are encoding in UTF-8 encoding. Reading in data with the UTF-8 encoding can be done by using the `encoding` argument within the `read.csv` function. 

If you know the encoding of your data, it is best to convert it to UTF-8 (either within R or elsewhere) before proceeding with the string matching algorithmns. If you are unsure of the encoding of your data, it is more important to read the data into R in its correct native encoding rather than to try and force the wrong encoding (eg. reading in data as UTF-8 when the data is encoded in Latin1). For those which are unfamiliar with encoding issues, an (optional) helper function within some string matching functions will try to guess the encoding of your data and fix it. For more information on the helper function, see the code in the github [here](https://github.com/datadrivenyale/climactor/blob/master/R/helper_functions.R). 

If you would like to learn more about encoding in R, you can refer to the Appendix (*Notes on encoding*) of this vignette or refer to several articles writtin on the topic by [Kunsttube](http://kunststube.net/encoding), [Kevin Ushey](http://kevinushey.github.io/), or [Maurício Collaça](https://rstudio-pubs-static.s3.amazonaws.com/279354_f552c4c41852439f910ad620763960b6.html). 

3 specific variables (name, country, entity type) are the most important while using the `climactor` package. Data preparation functions ensure that these variables are present and include the following functions: 

* `rename_col()` to rename data columns to the appropriate names 
* `add_country()` adds the country column to the dataset
* `add_entity_type()` adds the entity type column to the dataset
* `add_country_entity_type()` adds both the country and entity type columns to the dataset


### Rename columns with `rename_col()`

`rename_col()` checks the dataset for the requisite names and asks the user for columns to be renamed if the dataset do not contain the requisite names. This function ensures that column names are standardized so that downstream functions will work as intended. The only argument for the function is the user's dataset that is to be cleaned. 

```{r}
# Check before 
names(cdp)[1:5]
```

```{r, eval = F}
cdp <- rename_col(cdp)

```
```{r, echo = F, comment = ""}
# Note that because R Markdown does not really support interactive input, the 
# output that a user can expect for this dataset is printed here for user's reference 
cat("Please input the column name that contains the actors' names. \nInput \"skip\" if column does not exist.")
cat("Input:Organization")

cat("Please input the column name that contains the actors' entity types.\nInput \"skip\" if column does not exist.")
cat("Input:skip")

names(cdp)[grep("Organization", names(cdp))] <- "name"
```
```{r}
# Check names after
names(cdp)[1:5]
```

### Add country column with `add_country()`
`add_country()` first checks the dataset for the country column within the dataset provided. If the dataset does not contain a country column, then a country column will be initialized with default `NA` values. In this case, the country already exists in the cdp dataset. 
```{r, error = T}
cdp <- add_country(cdp)

```

### Add the entity type column with `add_entity_type()`
`add_entity_type()` first checks the dataset for the entity type column within the dataset provided. If the dataset does not contain and entity type column, then an entity type column will be initiatlized with a user specified input. In this case, we set the input as "City", since a majority of the actors within the CDP dataset are cities. 
```{r}
cdp <- add_entity_type(cdp, "City")
# Check the last few columns of the dataset
head(cdp[, (ncol(cdp)-5):ncol(cdp)])
```
### Add both country and entity type columns to the dataset with `add_country_entity_type()`
`add_country_entity_type()` constitutes a combination of the above two functions, and should be used if users know both country and entity type columns are missing from the dataset. Similarly, the function checks the dataset for both of these columns and does not add new columns if the columns are found within the dataset. If not, a country column is defined with default `NA` values, and an entity type column is defined with values from the `type` argument. In this case, both countries are already present in the cdp database and thus the function does not add any new columns. 
```{r}
ncol(cdp)
cdp <- add_country_entity_type(cdp)
ncol(cdp)
```

## Standardizing and preprocessing of data
After the preliminary preparation of data from the previous functions, we now move on to functions that start standardizing and preprocess the dataset from the user. This family of functions provide easy ways to further standardize the structure of your data and to make inferences based on your dataset that will help with steps in cleaning and matching your dataset to the key dictionaries in later functions.

### Fill in entity types for your data using `fill_type()`
Since the `add_entity_type()` function only allow users to input only one entity type for the entire dataset, the `fill_type()` function will allow users to update the entity type for users with datasets that contain more than one entity type. The function looks at the actor name and identifies common words within the actor's name to guess the entity type for that actor. 
```{r}
table(cdp$entity_type)
cdp <- fill_type(cdp)
table(cdp$entity_type)
# Just do a quick check for the ones listed as company
cdp$name[which(cdp$entity_type == "Company")]
```
The example here highlights the importance to conduct careful checks on the data after using the `fill_type()` function. The function picked up two actors which has the word "Corporation" in its name, but should really be cities (instead of companies). We manually fix the entity types for these actors. 
```{r}
cdp$entity_type[which(cdp$entity_type == "Company")] <- "City"
```

### Remove fill words from actor names using `remove_extra()`
The `remove_extra()` function removes extraneous words from the actors' names such as "Council", "District", "Province", etc etc. This helps to streamline the matching process used in downstream functions and provides better matches with the key dictionary. However, some of these words are used in the `fill_type()` function to determine the entity type of the actor, hence it is recommended that the `remove_extra()` function be used after the `fill_type()` function. 

```{r}
# Make a copy of the original names 
orignames <- cdp$name

# Apply remove_extra function
cdp <- remove_extra(cdp)

# Check for differences 
length(setdiff(orignames, cdp$name))
# 632 differences 

# For comparison's sake, look at some of the differences 
head(orignames[!orignames %in% cdp$name])
head(cdp$name[!cdp$name %in% orignames])

```

### Clean country names and add ISO data using `clean_country_iso()`
`clean_country_iso()` checks the countries in the supplied dataset and a supplied country dictionary using direct string matching and standardizes the country names according to the version in the country dictionary. The function allows users to input their own country dictionary (in the event that the user has a locally updated country dictionary, or their own one) but the package's included country dictionary can be called using `country_dict`. ISO2 and ISO3 data is available for including in one's own dataset, with users being able to select which version they prefer (with ISO3 being the default). Lastly, `clean_enc = F` checks the encoding of the country column of the user's dataset, guesses the correct encoding of the data, and tries to repair it if the data was read in with the wrong encoding. The default for this argument is `T` (checks not conducted), but the user should set it to `F` if they are unsure if they have read in the data in the correct encoding. A tell-tale sign is usually when unrecognized symbols or accents appear in the country names of the dataset. 
Apart from the dataset with the standardized country names, the `clean_country_iso()` also returns a `country_ind` vector which specifies which rows in the dataset will require further downstream cleaning of countries (countries not exactly matched to any entry in the country dictionary). 

```{r}
# Set clean_enc = F (not sure if encoding is "clean" for the sake of this example)
# Use package's country dictionary
# As usual, we store an original copy of the country names for comparison later on
origcountry <- cdp$Country
cdp <- clean_country_iso(cdp, country_dict, iso = 3, clean_enc = F)

```
We see that there are 21 countries in the cdp dataset that do not have an exact match in the country dictionary. The `country_ind` vector was created to show which rows in the datasets have countries where there was no exact matches in the country_dictionary. Let's examine these countries using the `country_ind` vector. 

```{r}
cdp$Country[country_ind]
```
It seems like there are two countries where there wasn't exact matches (Taiwan and Côte d'Ivoire).
These names will be matched in a later function, as detailed below. 

## String/Phonetic matching functions and cleaning of actor names 
We now almost have all that we need to start the process of cleaning actors' names in the dataset (the primary goal of the package). To do that, we first need to start with making sure all the country names in our dataset are standardized. This picks up from the previous section where we used exact string matching to clean the country names within the dataset. Now, we use fuzzy string matching to standardize and clean whatever country names are left to be cleaned. Then, the functions following which will attempt to clean the actors' names first through exact matching, then through phonetic string matching. 

### Use `fuzzify_country()` to ensure all countries in dataset are cleaned
This function is a continuation of the previous function `clean_country_iso()`, and should only be used 1) after the `clean_country_iso()` function has been used and 2) if there are any remaining countries that need to be cleaned (signaled by the presence of a `country_ind` vector). 
The function makes use of base R fuzzy matching algorithms to detect similar country names within the country dictionary to match to the user's input. The base R function in turn makes use of the Levenshtein distance to implement the fuzzy matching. The function will only do fuzzy matching for the rows which are in the `country_ind` vector, hence making the use of the `clean_country_iso()` function prior to the `fuzzify_country()` function crucial. 

The `fuzzify_country()` function also returns different vectors based on the different input specified by the user. A `custom_count` vector will be returned if users decided to input a custom country for any actors. An `unmatched_count` vector will be returned if users decided to skip any countries within their dataset, such that they can return to it later or input their own countries later on.  
It should also be further noted that the `fuzzify_country()` function will not match for `NA` values in the country column. The indices for these countries will still remain in the `country_ind` vector for users to easily check the name and entity types of those actors. The reason for this is to encourage users to manually check for those actors which countries are `NA` to ensure that these entries are valid. Users can opt to manually input a country after checking before using the functions to standardize the country names.   

```{r, eval = F}
cdp <- fuzzify_country(cdp, country_dict)
```
```{r, echo = F, comment = ""}
cat("[1] \"The original country is Taiwan, Greater China (Actor name: Yilan)\"
[1] \"Here are some possible matches we found: \"
1. Taiwan
 2. Macao
 3. Macao
 4. China
 5. Hong Kong
 6. Saint Helena
 7. Saint Martin
 8. Saint Martin
 9. Hong Kong
 10. Taiwan
 11. Papua New Guinea
 12. Saint Martin
 13. San Marino
 14. Bosnia and Herzegovina
 15. Bosnia and Herzegovina
Which name matches? Choose 1-15. (Type N if none match; type S to save your current progress)")
cat("Answer: 1")
cat("[1] \"Taiwan has been selected and will replace Taiwan, Greater China in the database.\"")
cat("[1] \"The original country is Côte d'Ivoire (Actor name: Commune de Bouaké)\"
[1] \"Here are some possible matches we found: \"
1. Cote d'Ivoire
 2. Cote d'Ivoire
 3. Cote d'Ivoire
 4. Cote d'Ivoire
 5. Cote d'Ivoire
 6. Cape Verde
 7. Andorra
 8. Belize
 9. Cambodia
 10. Cameroon
 11. Cape Verde
 12. Chile
 13. Costa Rica
 14. Ecuador
 15. Ecuador
Which name matches? Choose 1-15. (Type N if none match; type S to save your current progress)")
cat("Answer: 1")
cat("[1] \"Cote d'Ivoire has been selected and will replace Côte d'Ivoire in the database.\"")
country_short <- country_ind[!duplicated(cdp$Country[country_ind]) & !is.na(cdp$Country[country_ind])]
.count_updates <- data.frame(ind = country_short,
                             name = cdp$Country[country_short])

cdp$Country[cdp$Country == "Taiwan, Greater China"] <- "Taiwan"
cdp$Country[cdp$Country == "Côte d'Ivoire"] <- "Cote d'Ivoire"
country_ind <- which(!(cdp$country %in% country_dict$right))
```

We can do a quick check to make sure that all the country names are standardized based on the country dictionary. 
```{r}
sum(cdp$Country %in% country_dict$right)
# Quick check on country_ind 
country_ind
```
Now, with all the countries of the actors in the dataset all cleaned, we can go back to the clean_country_iso function again to add in the iso for the countries which were missed out because it was not cleaned. 
```{r}
# Check ISO first 
any(is.na(cdp$iso))
cdp$Country[is.na(cdp$iso)]

cdp <- clean_country_iso(cdp, country_dict, iso = 3, clean_enc = T) # We know the encoding is fine

any(is.na(cdp$iso)) # All have iso now 
```

### Resolve differences in entity types with `resolve_entity_types()`
Often, there are many actors that can be both a state and a city (examples abound - New York can refer to either New York State, or New York City, Kansas can refer to Kansas State, or Kansas City (in the state of Missouri), and so forth). The `resolve_entity_types()` function highlights such discrepancies between the user's dataset and the key dictionary to users. The function allows users to decide whether they would like to standardize the entity types to the key dictionaries' entries, or if they would like to get a dataframe highlighting where these differences lie (such that they can make any necessary edits themselves). 

Let's examine both cases, starting with the case where the user accepts the key dictionary's entity types. 
```{r, eval = F}
cdp_resolved <- resolve_entity_types(cdp, key_dict)
```
```{r, echo = F, comment = ""}
# Code to allow vignette to compile while showcasing the function 
name_iso_right <- which(paste0(cdp$name, cdp$iso) %in% paste0(key_dict$right, key_dict$iso))
name_iso_right_short <- which(unique(paste0(cdp$name, cdp$iso)) %in% paste0(key_dict$right, key_dict$iso))

# Coerce NA entity types to random character
cdp$entity_type[intersect(which(is.na(cdp$entity_type)), name_iso_right)] <- ","
# Check for wrong entity types in those that have correct names and iso
dict_ind <- na.omit(match(paste0(cdp$name[name_iso_right], cdp$iso[name_iso_right]),
                          paste0(key_dict$right, key_dict$iso)))
ent_ind <- name_iso_right[cdp$entity_type[name_iso_right] != key_dict$entity_type[dict_ind]]
dict_ent_ind <- dict_ind[cdp$entity_type[name_iso_right] != key_dict$entity_type[dict_ind]]
cat(paste0("We found ", length(ent_ind), " number of entries with the same actor name ",
           "and iso but conflicting entity types. Would you like to resolve all",
           " conflicts by accepting the key dictionary's entity type? (Y/N)"))
cat("Answer: Y")
cdp_resolved <- cdp
cdp_resolved$entity_type[ent_ind] <- key_dict$entity_type[dict_ent_ind]
```
We will examine the resolved entity types later on. For now, let's see what the returned dataframe looks like if the user opts to not accept all of the key dictionary's entity types. 
```{r, eval = F}
cdp <- resolve_entity_types(cdp, key_dict)
```
```{r, echo = F, comment = ""}
cat(paste0("We found ", length(ent_ind), " number of entries with the same actor name ",
           "and iso but conflicting entity types. Would you like to resolve all",
           " conflicts by accepting the key dictionary's entity type? (Y/N)"))
cat("Answer: N")
cat(paste0("A dataframe of the actors with conflicting entity types will be",
           "created for closer inspection of the conflicts."))
entity_conflicts <<- data.frame(name = cdp$name[ent_ind],
                                iso = cdp$iso[ent_ind],
                                user_entity = cdp$entity_type[ent_ind],
                                keydict_entity = key_dict$entity_type[dict_ent_ind],
                                user_index = ent_ind,
                                keydict_index = dict_ent_ind)
```
Now let's examine the outputs of the function 
```{r}
head(entity_conflicts)
head(cdp_resolved[entity_conflicts$user_index, c("name", "iso", "entity_type")])
```
Note that in the `cdp_resolved` dataset, the entity types have been changed to become the key dictionaries' entity type. In this case, we note that the entity type in the key dictionary is not necessarily what we want. We continue our data cleaning with the original cdp dataset's entity type.  

### Standardize actors' names with exact matching using `clean_name()`
We are now finally ready to start cleaning actors' names to standardized versions for further downstream analyses. The `clean_name()` function first checks for conflicting entity types (for actors with the same name and iso) between the dataset and the key dictionary, prompting users to choose which version they think is correct. The function then checks the actors' names within the dataset for exact matches in the supplied key dictionary, and replaces it with the standardized version if the original name is already present in the supplied key dictionary. 
Similar to the `clean_country_iso()` function, users can supply their own version of the key dictionary (in the event that they have a locally updated version, for example). Otherwise, the package's included key dictionary can be called using `key_dict`.  The `clean_enc` argument also works similarly to the `clean_country_iso()` function, where if it is `F` (false), then the function checks the encoding of the name column within the user's dataset, guesses the correct encoding, and tries to repair it if the data was read in with the wrong encoding. The argument defaults to `T` (true). Apart from the dataset with the standardized actors' names, the `clean_name()` function also returns an `indices` vector, which specifies which rows in the dataset will require further downstream data cleaning (actors' names cannot be exactly matched to key dictionary). 

```{r}
precleaned_names <- cdp$name
cdp <- clean_name(cdp, key_dict, clean_enc = F)

sum(!precleaned_names %in% cdp$name) # 344 cleaned names 
# Compare the differences between the pre-cleaned and cleaned names  
head(precleaned_names[!precleaned_names %in% cdp$name]) 
head(cdp$name[!precleaned_names %in% cdp$name])
```
Note also that the function created a new vector named `indices` in the global environment. This vector denotes the indices of those actors' names which were not cleaned (due to the lack of an exact match in the key dictionary) by the `clean_name()` function. This vector will be used later on to determine which names need to be cleaned using phonetic matching in the `phonetify_names()` function. 

### Standardize actors' names with phonetic string matching using `phonetify_names()` 
The final step of the cleaning process involves the use of phonetic string matching algorithms to standardize actors' names with names that are similar but not exact to the actors present in our key dictionary. The function works by first taking the indices vector created from the `clean_name()` function (or creating one if no such vector exists), that indicates which actors' names require cleaning. 

The function then takes Anglicized versions of the actors' names and create phonetic representations of the name. To ensure accurate matches, a combination of 5 different algorithms are used to create multiple phonetic representations of the name (Metaphone, Nysiis modified, Onca modified refined, Phonex, Roger Root). For more details, see `?phonics::phonics`.

The string distance between the phonetic representations of the actor name is then compared to the corresponding phonetic representations of names within the key dictionary. Likewise, the string distance is also computed using a variety of methods (Full Damerau-Levenshtein distance, q-gram distance, cosine distance (between q-gram profiles), Jaccard distance between (q-gram profiles), and Jaro-Winker distance). For more details, see ```` ?stringdist::`stringdist-metrics` ```` (note the backticks). 

Finally, the top 15 (if applicable) distance scores between the user-input actor's name and the entries in our key dictionary are presented for the user to select from. Along the way, should users choose to not match any actor names (ie. to skip them), or to input their own custom names, the `unmatched_indices` and `custom_indices` will be generated respectively to note user's input for these names and allow for further inspection. 

For the sake of this vignette, we will not clean the entire dataset. Instead, the below examples are selected to show the various options that users have when using the function, and how to proceed in each case. 

To call the function, simply call `phonetify_names()` with the dataset that needs to be cleaned and the dictionary to clean it with. The function will return a cleaned dataset, so be sure to assign that to a name in order to store the results. We consider a few examples from the function below. 

```{r, eval = F}
cdp <- phonetify_names(cdp, key_dict)

```

The first example considers an actor which is currently not in the key dict, with which the user does not want to enter a custom name for. 
```{r, echo = F, comment = ""}
cat("The original name is Prefeitura de Rio Verde with iso code BRA and entity type Region
Here are some possible matches we found: 
1:  Pernambuco; iso: BRA and entity type: Region
2:  Rio de Janeiro (State); iso: BRA and entity type: Region
3:  São Paulo State; iso: BRA and entity type: Region
4:  Minas Gerais; iso: BRA and entity type: Region
5:  Rio Grande do Sul; iso: BRA and entity type: Region
6:  Ceara; iso: BRA and entity type: Region
7:  Acre; iso: BRA and entity type: Region
8:  Amapa; iso: BRA and entity type: Region
9:  Mato Grosso; iso: BRA and entity type: Region
10: Rondônia; iso: BRA and entity type: Region
11: Tocantins; iso: BRA and entity type: Region
12: Amazonas, Brazil; iso: BRA and entity type: Region
Which name matches? Choose 1-15. (Type N if none match; type S to save your current progress) ")
cat("Answer: n")
cat("Do you want to enter a custom name instead? (Y/N)")
cat("Answer: n")
cat("The previous name (Prefeitura de Rio Verde) will be kept.")
unmatched_indices <- indices[1]
```
The second example considers when a match is found within the key dictionary for the actor being cleaned. 
```{r, echo = F, comment = ""}
cat("The original name is Broward , FL with iso code USA and entity type City
Here are some possible matches we found: 
1:  Broward County, FL; iso: USA and entity type: City
2:  Brighton, NY; iso: USA and entity type: City
3:  Bedford, NY; iso: USA and entity type: City
4:  Bridgeport, CT; iso: USA and entity type: City
5:  Brisbane, CA; iso: USA and entity type: City
6:  Providence, RI; iso: USA and entity type: City
7:  Brownsville, TX; iso: USA and entity type: City
8:  West Chester, PA; iso: USA and entity type: City
9:  Beaufort, SC; iso: USA and entity type: City
10: Boulder, CO; iso: USA and entity type: City
11: Beaverton, OR; iso: USA and entity type: City
12: Burnsville, MN; iso: USA and entity type: City
13: Bakersfield, CA; iso: USA and entity type: City
14: Bellevue, ID; iso: USA and entity type: City
15: Bayfield, WI; iso: USA and entity type: City
Which name matches? Choose 1-15. (Type N if none match; type S to save your current progress)")
cat("Answer: 1")
cat("Broward County, FL has been selected and will replace Broward , FL in the database.")
cdp$name[grep("Broward , FL", cdp$name)] <- "Broward County, FL"
```
If we wanted to include a custom name that was not listed as an option, we could also do so. 
```{r, echo = F, comment = ""}
cat("The original name is Prefeitura  de Canoas with iso code BRA and entity type Region
Here are some possible matches we found: 
1:  Pernambuco; iso: BRA and entity type: Region
2:  Rio de Janeiro (State); iso: BRA and entity type: Region
3:  Minas Gerais; iso: BRA and entity type: Region
4:  São Paulo State; iso: BRA and entity type: Region
5:  Rio Grande do Sul; iso: BRA and entity type: Region
6:  Ceara; iso: BRA and entity type: Region
7:  Tocantins; iso: BRA and entity type: Region
8:  Mato Grosso; iso: BRA and entity type: Region
9:  Acre; iso: BRA and entity type: Region
10: Amapa; iso: BRA and entity type: Region
11: Rondônia; iso: BRA and entity type: Region
12: Amazonas, Brazil; iso: BRA and entity type: Region
Which name matches? Choose 1-15. (Type N if none match; type S to save your current progress)")
cat("Answer: n")
cat("Do you want to enter a custom name instead? (Y/N)")
cat("Answer: y")
cat("Enter in custom name: Canoas")
cat("The name (Canoas) will be kept in the dataset.
The name has not been added to the key dictionary yet but can be added with the update_key_dict function.
The row number of the custom name has been added to a vector called custom_indices.")
custom_indices <- indices[3]
head(indices)
```
The index of the entry will then be recorded for user's future inspection. 
To stop the `phonetify_names()` function, we can simply input the letter s.
```{r, echo = F, comment = ""}
cat("The original name is Município de Odemira with iso code PRT and entity type City
Here are some possible matches we found: 
1:  Odemira; iso: PRT and entity type: City
2:  Santarém; iso: PRT and entity type: City
3:  Moita; iso: PRT and entity type: City
4:  Guimarães; iso: PRT and entity type: City
5:  Mirandela; iso: PRT and entity type: City
6:  Faro; iso: PRT and entity type: City
7:  Ovar; iso: PRT and entity type: City
8:  Câmara de Lobos; iso: PRT and entity type: City
9:  Torres Vedras; iso: PRT and entity type: City
10: Águeda; iso: PRT and entity type: City
11: Portalegre; iso: PRT and entity type: City
12: Braga; iso: PRT and entity type: City
13: Coruche; iso: PRT and entity type: City
14: Montalegre; iso: PRT and entity type: City
15: Município da Batalha; iso: PRT and entity type: City
Which name matches? Choose 1-15. (Type N if none match; type S to save your current progress) ")
cat("Answer: s")
cat("Your current progress will be returned.
If it's not saved, be sure you saved your results in a variable with an assign function")
```
We can take a closer look at the unmatched and custom entries 
```{r}
cdp[unmatched_indices, c("name", "iso", "entity_type")]
cdp[custom_indices, c("name", "iso", "entity_type")]
```

To resume cleaning of the data, users can simply ensure that the `unmatched_indices` and `custom_indices` vector are loaded into the R workspace. The presence of these vectors will allow users to pick up where they left off in cleaning the dataset. As such, should users need to close the R file and continue cleaning the data in the future, users should ensure that the two vectors are saved (either as csv files or RDS data files) to be loaded for future use. 

## Post Processing Functions
Congratulations! Your dataset should be cleaned by this step. 

Apart from the usual saving and exporting of data after the cleaning process, users can also opt to merge in contextual data for their actors from the contextuals database curated by Data-Driven Lab (see `?contextualize_data()` for details on what contextual data is available). Furthermore, users can also choose to update the key and country dictionaries with the actors from their database that were not present in the respective dictionaries. These functions then allow users to store a local, expanded copy of the dictionaries for future use. Users can also submit the updated dictionaries to **datadrivenlab@gmail.com** for incorporation into the package dictionaries for use by others. 

As described above, the `contextualize_data()` function allows users to merge contextual data (where available) to the actors in the user's dataset. 

The functions `update_key_dict()` and `update_country_dict()` allow users to update the key and country dictionaries respectively. 

### Include contextuals information with `contextualize_data()`
The `climactor` package also includes a contextual database (named `contextuals`) that includes contextual information for some of the subnational actors that has made climate commitments. This contextual data can be merged into the user's dataset using the `contextualize_data()` function. The contextual database contains the following data:

- `name`: Name of the actor
- `iso`: 3-letter ISO of the country of actor
- `entity_type`: Entity type of the actor 
- `country`: Country of actor
- `region`: Broader geographical region the actor resides in 
- `area`: Size (geographical area) of the actor
- `area_units`: Units for the `area` column (km$^2$, etc.)
- `initiatives_committed`: Names of climate initiatives which actor commits to (separated by a semi-colon)
- `num_commit`: Number of initiatives actor commits to
- `lat`: Geographical latitude of actor (in decimal degrees)
- `lng`: Geographical longitude of actor (in decimal degrees)
- `population`: Population of actor
- `population_year`: Year of recorded population 
- `state`: State/Province which actor is in 
- `gdp`: GDP of actor
- `gdp_unit`: Units which GDP data is in 
- `gdp_year`: Year of recorded GDP 

The user can specify which of the contextuals information you would like merged into your own database using the `context` argument. 

In this example, let's merge in the `initiatives_committed` and `num_commit` columns. 

```{r}
cdp_merged <- contextualize_data(cdp, contextuals, context = c("initiatives_committed",
                                                               "num_commit"))
# Let's take a look at the data 
head(cdp_merged[, c("name", "iso", "entity_type", "initiatives_committed", 
                    "num_commit")])
```

### Updating the country dictionary with `update_country_dict()`

If the user's dataset contains country names which were not included in the `country_dict`, a local version of the `country_dict` can be created for use in future data cleaning iterations. This version of the `country_dict` can also be sent to the Data-Driven Lab team at **datadrivenlab@gmail.com**, where the team will check the updated dictionary and update the dictionary accordingly. 

The `update_country_dict` function takes in two compulsory arguments and two optional arguments. The two compulsory arguments are the dataset and the country dictionary that was used to clean the dataset, and the two optional arguments are the `custom_count` and `unmatched_count` vectors. Country names that were user's custom input and also unmatched country names will automatically be added to the `wrong` column in the updated country dictionary. 

```{r}
local_country_dict <- update_country_dict(cdp, country_dict)

# Check for the newly added entries
local_country_dict[!(local_country_dict$wrong %in% country_dict$wrong), ]
```

### Updating the key dictionary with `update_key_dict()` 
